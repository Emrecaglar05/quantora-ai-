# Risk ve Uyum AjanÄ± (Node MantÄ±ÄŸÄ±).
print("COMPLIANCE MODULE LOADED")

from langchain_core.messages import HumanMessage
from app.core.config import get_llm
from app.core.prompts import COMPLIANCE_PROMPT

# Bu ajan denetÃ§i olduÄŸu iÃ§in temperature 0 olmalÄ± (Kesin ve YaratÄ±cÄ±sÄ±z)
llm = get_llm(temperature=0.0)

def compliance_node(state: dict):
    """
    Risk ve Uyum AjanÄ±:
    SorumluluÄŸu: Analistin cevabÄ±nÄ± etik, yasal ve gÃ¼venlik aÃ§Ä±sÄ±ndan denetlemek.
    """
    print("--- RÄ°SK KONTROLÃœ YAPILIYOR ---")
    
    # Analistin son Ã¼rettiÄŸi cevabÄ± al
    last_message = state["messages"][-1]
    last_content = last_message.content

    # Denetim iÃ§in Ã¶zel bir prompt hazÄ±rla
    check_prompt = f"""
    {COMPLIANCE_PROMPT}

    AÅŸaÄŸÄ±daki metni yukarÄ±daki kurallara gÃ¶re denetle:
    ---
    {last_content}
    ---
    """

    # LLM'e sor
    response = llm.invoke([HumanMessage(content=check_prompt)])
    result = response.content.strip()

    # Sonucu Analiz Et (APPROVED veya REJECTED)
    if "APPROVED" in result:
        print("âœ… ONAYLANDI")
        return {
            "compliance_status": "APPROVED",
            "feedback": "",
            "sender": "Compliance_Agent"
        }
    else:
        # "REJECTED | Sebebini al"
        parts = result.split("|", 1)
        reason = parts[1].strip() if len(parts) > 1 else "Genel gÃ¼venlik ihlali."
        print(f"ðŸ›‘ REDDEDÄ°LDÄ°: {reason}")
        
        return {
            "compliance_status": "REJECTED", 
            "feedback": reason,
            "sender": "Compliance_Agent"
        }